{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from word2vec.src.train import train\n",
    "from word2vec.src.utils import load_yaml"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model': 'SkipGram',\n 'model_dir': 'artifacts',\n 'data_dir': 'data',\n 'epochs': 10,\n 'batch_size': 64,\n 'learning_rate': 0.05,\n 'checkpoint_frequency': 1,\n 'steps_train': 60,\n 'steps_val': 60,\n 'n_words': 4,\n 'min_word_frequency': 50,\n 'max_sequence_length': 256,\n 'embed_size': 50,\n 'embed_max_norm': 1}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_yaml('config.yaml')\n",
    "config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4099\n",
      "Adjusting learning rate of group 0 to 5.0000e-02.\n",
      "Epoch 1 / 10. Train loss: 47.9278, Validation loss: 32.2594\n",
      "Adjusting learning rate of group 0 to 4.5000e-02.\n",
      "Epoch 2 / 10. Train loss: 44.7503, Validation loss: 31.9854\n",
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "Epoch 3 / 10. Train loss: 44.3496, Validation loss: 31.7903\n",
      "Adjusting learning rate of group 0 to 3.5000e-02.\n",
      "Epoch 4 / 10. Train loss: 44.2255, Validation loss: 31.2724\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 5 / 10. Train loss: 44.0558, Validation loss: 31.8481\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Epoch 6 / 10. Train loss: 43.8478, Validation loss: 31.6540\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 7 / 10. Train loss: 43.7491, Validation loss: 31.7971\n",
      "Adjusting learning rate of group 0 to 1.5000e-02.\n",
      "Epoch 8 / 10. Train loss: 43.4695, Validation loss: 31.5692\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 9 / 10. Train loss: 43.5581, Validation loss: 31.7264\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Epoch 10 / 10. Train loss: 43.5415, Validation loss: 31.5857\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Model artifacts are saved to folder: artifacts\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Demo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(f\"{config['model_dir']}/model.pt\", map_location=device)\n",
    "vocab = torch.load(f\"{config['model_dir']}/vocab.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "SkipGram(\n  (embeddings): Embedding(4099, 50, max_norm=1)\n  (linear): Linear(in_features=50, out_features=4099, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0     0.004864  0.058226  0.102455 -0.068684 -0.110115  0.157051 -0.022095   \n1    -0.124084 -0.066151  0.154998 -0.069169  0.012323  0.069352 -0.069914   \n2     0.031894  0.043751 -0.105125  0.006367  0.057542  0.010927 -0.036122   \n3     0.249401  0.041974  0.005398 -0.012846  0.036359  0.100475 -0.125695   \n4     0.046675  0.128045 -0.126386  0.017677 -0.086506 -0.026073  0.068903   \n...        ...       ...       ...       ...       ...       ...       ...   \n4094 -0.199458  0.127758  0.004696 -0.151444  0.196218  0.137440  0.119755   \n4095  0.009298  0.052724  0.057545  0.055180  0.101406  0.047301  0.074818   \n4096 -0.089679  0.117325  0.083674  0.061179 -0.234210  0.127248 -0.117865   \n4097 -0.226796  0.133705  0.016640  0.089745  0.021849  0.014482 -0.232020   \n4098 -0.173412  0.176645  0.056175 -0.177659 -0.049073  0.086972  0.284275   \n\n            7         8         9   ...        40        41        42  \\\n0    -0.100077 -0.240014  0.028700  ... -0.051605 -0.046651 -0.065859   \n1    -0.011487 -0.004807  0.006637  ... -0.031389  0.026722 -0.027680   \n2    -0.153236 -0.356225  0.000016  ...  0.071755 -0.013104 -0.309857   \n3    -0.022251 -0.111198  0.067785  ... -0.060293  0.112995 -0.001086   \n4    -0.230947  0.108470  0.149829  ... -0.247969 -0.004013 -0.243505   \n...        ...       ...       ...  ...       ...       ...       ...   \n4094 -0.103406 -0.065517 -0.059096  ...  0.000133 -0.248536 -0.191121   \n4095 -0.028003 -0.195690 -0.169775  ... -0.202503  0.006349 -0.088079   \n4096 -0.072367 -0.073205  0.162165  ... -0.080407 -0.067745 -0.050152   \n4097 -0.129789 -0.055314  0.121525  ... -0.099888 -0.240493 -0.179643   \n4098 -0.082025 -0.148041  0.028577  ... -0.097876 -0.030157 -0.103413   \n\n            43        44        45        46        47        48        49  \n0     0.013713 -0.001473 -0.008831  0.059250  0.093462 -0.212240  0.030380  \n1    -0.039143  0.058650 -0.099987 -0.047607  0.142750 -0.168110  0.085693  \n2     0.042005  0.083636  0.070455 -0.022662  0.031165 -0.140603 -0.032430  \n3     0.010267 -0.014071  0.132088 -0.088297  0.021666 -0.533064  0.004619  \n4     0.216038 -0.222477  0.161965  0.075252  0.132246 -0.013527  0.103756  \n...        ...       ...       ...       ...       ...       ...       ...  \n4094 -0.064573 -0.024117 -0.024690 -0.242933  0.131435 -0.051108  0.070901  \n4095 -0.077961 -0.016131 -0.173869  0.283655  0.135124 -0.108162 -0.108959  \n4096 -0.161416 -0.143546  0.154494  0.059882  0.054609 -0.073639  0.124874  \n4097 -0.114220 -0.062328 -0.014637 -0.080130  0.043271 -0.143201 -0.018509  \n4098 -0.120398 -0.079220 -0.082959 -0.143308  0.059302 -0.064934  0.088119  \n\n[4099 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004864</td>\n      <td>0.058226</td>\n      <td>0.102455</td>\n      <td>-0.068684</td>\n      <td>-0.110115</td>\n      <td>0.157051</td>\n      <td>-0.022095</td>\n      <td>-0.100077</td>\n      <td>-0.240014</td>\n      <td>0.028700</td>\n      <td>...</td>\n      <td>-0.051605</td>\n      <td>-0.046651</td>\n      <td>-0.065859</td>\n      <td>0.013713</td>\n      <td>-0.001473</td>\n      <td>-0.008831</td>\n      <td>0.059250</td>\n      <td>0.093462</td>\n      <td>-0.212240</td>\n      <td>0.030380</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.124084</td>\n      <td>-0.066151</td>\n      <td>0.154998</td>\n      <td>-0.069169</td>\n      <td>0.012323</td>\n      <td>0.069352</td>\n      <td>-0.069914</td>\n      <td>-0.011487</td>\n      <td>-0.004807</td>\n      <td>0.006637</td>\n      <td>...</td>\n      <td>-0.031389</td>\n      <td>0.026722</td>\n      <td>-0.027680</td>\n      <td>-0.039143</td>\n      <td>0.058650</td>\n      <td>-0.099987</td>\n      <td>-0.047607</td>\n      <td>0.142750</td>\n      <td>-0.168110</td>\n      <td>0.085693</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.031894</td>\n      <td>0.043751</td>\n      <td>-0.105125</td>\n      <td>0.006367</td>\n      <td>0.057542</td>\n      <td>0.010927</td>\n      <td>-0.036122</td>\n      <td>-0.153236</td>\n      <td>-0.356225</td>\n      <td>0.000016</td>\n      <td>...</td>\n      <td>0.071755</td>\n      <td>-0.013104</td>\n      <td>-0.309857</td>\n      <td>0.042005</td>\n      <td>0.083636</td>\n      <td>0.070455</td>\n      <td>-0.022662</td>\n      <td>0.031165</td>\n      <td>-0.140603</td>\n      <td>-0.032430</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.249401</td>\n      <td>0.041974</td>\n      <td>0.005398</td>\n      <td>-0.012846</td>\n      <td>0.036359</td>\n      <td>0.100475</td>\n      <td>-0.125695</td>\n      <td>-0.022251</td>\n      <td>-0.111198</td>\n      <td>0.067785</td>\n      <td>...</td>\n      <td>-0.060293</td>\n      <td>0.112995</td>\n      <td>-0.001086</td>\n      <td>0.010267</td>\n      <td>-0.014071</td>\n      <td>0.132088</td>\n      <td>-0.088297</td>\n      <td>0.021666</td>\n      <td>-0.533064</td>\n      <td>0.004619</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.046675</td>\n      <td>0.128045</td>\n      <td>-0.126386</td>\n      <td>0.017677</td>\n      <td>-0.086506</td>\n      <td>-0.026073</td>\n      <td>0.068903</td>\n      <td>-0.230947</td>\n      <td>0.108470</td>\n      <td>0.149829</td>\n      <td>...</td>\n      <td>-0.247969</td>\n      <td>-0.004013</td>\n      <td>-0.243505</td>\n      <td>0.216038</td>\n      <td>-0.222477</td>\n      <td>0.161965</td>\n      <td>0.075252</td>\n      <td>0.132246</td>\n      <td>-0.013527</td>\n      <td>0.103756</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4094</th>\n      <td>-0.199458</td>\n      <td>0.127758</td>\n      <td>0.004696</td>\n      <td>-0.151444</td>\n      <td>0.196218</td>\n      <td>0.137440</td>\n      <td>0.119755</td>\n      <td>-0.103406</td>\n      <td>-0.065517</td>\n      <td>-0.059096</td>\n      <td>...</td>\n      <td>0.000133</td>\n      <td>-0.248536</td>\n      <td>-0.191121</td>\n      <td>-0.064573</td>\n      <td>-0.024117</td>\n      <td>-0.024690</td>\n      <td>-0.242933</td>\n      <td>0.131435</td>\n      <td>-0.051108</td>\n      <td>0.070901</td>\n    </tr>\n    <tr>\n      <th>4095</th>\n      <td>0.009298</td>\n      <td>0.052724</td>\n      <td>0.057545</td>\n      <td>0.055180</td>\n      <td>0.101406</td>\n      <td>0.047301</td>\n      <td>0.074818</td>\n      <td>-0.028003</td>\n      <td>-0.195690</td>\n      <td>-0.169775</td>\n      <td>...</td>\n      <td>-0.202503</td>\n      <td>0.006349</td>\n      <td>-0.088079</td>\n      <td>-0.077961</td>\n      <td>-0.016131</td>\n      <td>-0.173869</td>\n      <td>0.283655</td>\n      <td>0.135124</td>\n      <td>-0.108162</td>\n      <td>-0.108959</td>\n    </tr>\n    <tr>\n      <th>4096</th>\n      <td>-0.089679</td>\n      <td>0.117325</td>\n      <td>0.083674</td>\n      <td>0.061179</td>\n      <td>-0.234210</td>\n      <td>0.127248</td>\n      <td>-0.117865</td>\n      <td>-0.072367</td>\n      <td>-0.073205</td>\n      <td>0.162165</td>\n      <td>...</td>\n      <td>-0.080407</td>\n      <td>-0.067745</td>\n      <td>-0.050152</td>\n      <td>-0.161416</td>\n      <td>-0.143546</td>\n      <td>0.154494</td>\n      <td>0.059882</td>\n      <td>0.054609</td>\n      <td>-0.073639</td>\n      <td>0.124874</td>\n    </tr>\n    <tr>\n      <th>4097</th>\n      <td>-0.226796</td>\n      <td>0.133705</td>\n      <td>0.016640</td>\n      <td>0.089745</td>\n      <td>0.021849</td>\n      <td>0.014482</td>\n      <td>-0.232020</td>\n      <td>-0.129789</td>\n      <td>-0.055314</td>\n      <td>0.121525</td>\n      <td>...</td>\n      <td>-0.099888</td>\n      <td>-0.240493</td>\n      <td>-0.179643</td>\n      <td>-0.114220</td>\n      <td>-0.062328</td>\n      <td>-0.014637</td>\n      <td>-0.080130</td>\n      <td>0.043271</td>\n      <td>-0.143201</td>\n      <td>-0.018509</td>\n    </tr>\n    <tr>\n      <th>4098</th>\n      <td>-0.173412</td>\n      <td>0.176645</td>\n      <td>0.056175</td>\n      <td>-0.177659</td>\n      <td>-0.049073</td>\n      <td>0.086972</td>\n      <td>0.284275</td>\n      <td>-0.082025</td>\n      <td>-0.148041</td>\n      <td>0.028577</td>\n      <td>...</td>\n      <td>-0.097876</td>\n      <td>-0.030157</td>\n      <td>-0.103413</td>\n      <td>-0.120398</td>\n      <td>-0.079220</td>\n      <td>-0.082959</td>\n      <td>-0.143308</td>\n      <td>0.059302</td>\n      <td>-0.064934</td>\n      <td>0.088119</td>\n    </tr>\n  </tbody>\n</table>\n<p>4099 rows × 50 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.detach().numpy()\n",
    "df_embeddings = pd.DataFrame(embeddings)\n",
    "df_embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "df_embeddings_transformed = pd.DataFrame(tsne.fit_transform(df_embeddings))\n",
    "df_embeddings_transformed.index = vocab.get_itos()\n",
    "df_embeddings_transformed.rename(columns={0: 'x', 1: 'y'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_embeddings_plot = df_embeddings_transformed.sample(50, replace=False)\n",
    "\n",
    "fig = px.scatter(df_embeddings_plot, x='x', y='y', text=df_embeddings_plot.index)\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text='Visualisation of words using t-SNE algorithm'\n",
    ")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find similar words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_embedding(df_embeddings, vocab, word: str) -> str:\n",
    "    return df_embeddings.loc[vocab.get_stoi()[word]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def top_similar_words(df_embeddings: pd.DataFrame, word: str | list[float], vocab = None, n: int = 10) -> dict[str, float]:\n",
    "    if isinstance(word, str):\n",
    "        word_id = vocab[word]\n",
    "        word_vec = df_embeddings.loc[word_id]\n",
    "    else:\n",
    "        word_id = -1\n",
    "        word_vec = word\n",
    "    distances = df_embeddings.apply(lambda embed: np.matmul(word_vec, embed), axis=1)\n",
    "    if isinstance(word, str):\n",
    "        distances = distances.drop(word_id)\n",
    "    similar_distances = distances.sort_values(ascending=False).iloc[:n]\n",
    "    return {vocab.get_itos()[id]: similar_distances[id] for id in similar_distances.index}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'think': 0.9609865,\n 'really': 0.94858843,\n 'want': 0.94356865,\n 'don': 0.8979643,\n 'feel': 0.8942174,\n 'someone': 0.891913,\n 'do': 0.8917615,\n 'you': 0.8869977,\n 'doesn': 0.883993,\n 'believe': 0.87273043}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similar_words(df_embeddings, 'know', vocab, n=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector equations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "embed_1 = get_embedding(df_embeddings, vocab, 'man')\n",
    "embed_2 = get_embedding(df_embeddings, vocab, 'woman')\n",
    "embed_3 = get_embedding(df_embeddings, vocab, 'king')\n",
    "embed_4 = embed_3 - embed_1 + embed_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'djedkare': 0.97340596,\n 'king': 0.9723966,\n 'christ': 0.9478258,\n 'lord': 0.93334126,\n 'st': 0.9236278,\n 'mary': 0.9071252,\n 'gerard': 0.904146,\n 'alfred': 0.8956475,\n 'catholic': 0.8883756,\n 'biographer': 0.8828482}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = top_similar_words(df_embeddings, embed_4, vocab, n=10)\n",
    "top_n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
